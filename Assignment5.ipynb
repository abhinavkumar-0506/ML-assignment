{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ASSIGNMENT-5**\n",
        "\n",
        "Q1\n"
      ],
      "metadata": {
        "id": "q6sgH3xFyx2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "n = 500\n",
        "\n",
        "X1 = np.random.randn(n)\n",
        "X2 = X1 + np.random.randn(n)*0.1\n",
        "X3 = X1*0.5 + np.random.randn(n)*0.1\n",
        "X4 = X2 + np.random.randn(n)*0.2\n",
        "X5 = X3*1.2 + np.random.randn(n)*0.1\n",
        "X6 = X1 + X3 + np.random.randn(n)*0.1\n",
        "X7 = X2 - X3 + np.random.randn(n)*0.1\n",
        "\n",
        "X = np.column_stack([X1,X2,X3,X4,X5,X6,X7])\n",
        "y = 4*X1 + 2*X2 - 3*X3 + np.random.randn(n)*0.2\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "def ridge_gradient_descent(X, y, lr, lambd, iterations=1000):\n",
        "    m, n = X.shape\n",
        "    w = np.zeros(n)\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        y_pred = np.dot(X, w)\n",
        "        gradient = (1/m) * np.dot(X.T, (y_pred - y)) + (lambd/m) * w\n",
        "        w = w - lr * gradient\n",
        "\n",
        "    return w\n",
        "\n",
        "\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "lambdas = [1e-15, 1e-10, 1e-5, 1e-3, 0.1, 1, 10, 20]\n",
        "\n",
        "best_r2 = -999\n",
        "best_set = None\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for lam in lambdas:\n",
        "        w = ridge_gradient_descent(X_train, y_train, lr, lam)\n",
        "        y_pred = np.dot(X_test, w)\n",
        "\n",
        "        if np.isnan(y_pred).any() or np.isinf(y_pred).any():\n",
        "            continue\n",
        "\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        if r2 > best_r2:\n",
        "            best_r2 = r2\n",
        "            best_set = (lr, lam, r2)\n",
        "\n",
        "print(\"Best LR:\", best_set[0])\n",
        "print(\"Best Lambda:\", best_set[1])\n",
        "print(\"Best R2 Score:\", best_set[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMzf_4mSy_M3",
        "outputId": "facd669b-80d7-4201-abdf-4458bde2dc3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best LR: 0.1\n",
            "Best Lambda: 1e-15\n",
            "Best R2 Score: 0.9958296100774573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-748070970.py:32: RuntimeWarning: invalid value encountered in subtract\n",
            "  w = w - lr * gradient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2"
      ],
      "metadata": {
        "id": "cGAbL4Uy1bP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/Hitters (1).csv\")\n",
        "\n",
        "# (a)\n",
        "df = df.dropna()\n",
        "\n",
        "\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# (b)\n",
        "X = df.drop(\"Salary\", axis=1)\n",
        "y = df[\"Salary\"]\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# (c)\n",
        "lin = LinearRegression().fit(X_train, y_train)\n",
        "ridge = Ridge(alpha=0.5748).fit(X_train, y_train)\n",
        "lasso = Lasso(alpha=0.5748, max_iter=5000).fit(X_train, y_train)\n",
        "\n",
        "\n",
        "models = {\n",
        "    \"Linear Regression\": lin,\n",
        "    \"Ridge Regression\": ridge,\n",
        "    \"LASSO Regression\": lasso\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(name, \" → R2 Score:\", r2_score(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxcC7bJs1dDw",
        "outputId": "7456cd1a-bd06-4c7a-bb30-6049a4a5076a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression  → R2 Score: 0.29074518557981444\n",
            "Ridge Regression  → R2 Score: 0.2997888803309703\n",
            "LASSO Regression  → R2 Score: 0.29942440974749995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3"
      ],
      "metadata": {
        "id": "JNKiD4BC7yuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "ridge_cv = RidgeCV(alphas=[0.1, 1, 10, 50, 100])\n",
        "ridge_cv.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "lasso_cv = LassoCV(alphas=[0.1, 1, 10, 50, 100], max_iter=5000)\n",
        "lasso_cv.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Ridge Alpha:\", ridge_cv.alpha_)\n",
        "print(\"Best Lasso Alpha:\", lasso_cv.alpha_)\n",
        "\n",
        "print(\"RidgeCV R2 Score:\", r2_score(y_test, ridge_cv.predict(X_test)))\n",
        "print(\"LassoCV R2 Score:\", r2_score(y_test, lasso_cv.predict(X_test)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73t9bqUI1aSp",
        "outputId": "cef9c296-f689-4571-edae-a988b4a72a9a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Ridge Alpha: 1.0\n",
            "Best Lasso Alpha: 0.1\n",
            "RidgeCV R2 Score: 0.5758185345428238\n",
            "LassoCV R2 Score: 0.4814202815313765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4"
      ],
      "metadata": {
        "id": "6QmZzUXQ8hOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "def train_binary_logistic(X, y, lr=0.1, iterations=2000):\n",
        "    m, n = X.shape\n",
        "    w = np.zeros(n)\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        z = np.dot(X, w)\n",
        "        h = sigmoid(z)\n",
        "        gradient = np.dot(X.T, (h - y)) / m\n",
        "        w -= lr * gradient\n",
        "\n",
        "    return w\n",
        "\n",
        "\n",
        "classes = np.unique(y)\n",
        "weights = []\n",
        "\n",
        "for c in classes:\n",
        "    y_binary = (y_train == c).astype(int)\n",
        "    w = train_binary_logistic(X_train, y_binary)\n",
        "    weights.append(w)\n",
        "\n",
        "weights = np.array(weights)\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def train_binary_logistic(X, y, lr=0.1, iterations=2000):\n",
        "    m, n = X.shape\n",
        "    w = np.zeros(n)\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        z = np.dot(X, w)\n",
        "        h = sigmoid(z)\n",
        "        gradient = np.dot(X.T, (h - y)) / m\n",
        "        w -= lr * gradient\n",
        "\n",
        "    return w\n",
        "\n",
        "classes = np.unique(y)\n",
        "weights = []\n",
        "\n",
        "for c in classes:\n",
        "    y_binary = (y_train == c).astype(int)\n",
        "    w = train_binary_logistic(X_train, y_binary)\n",
        "    weights.append(w)\n",
        "\n",
        "weights = np.array(weights)\n",
        "\n",
        "\n",
        "preds = []\n",
        "for x in X_test:\n",
        "    scores = [np.dot(w, x) for w in weights]\n",
        "    preds.append(np.argmax(scores))\n",
        "\n",
        "acc = accuracy_score(y_test, preds)\n",
        "print(\"OVR Logistic Regression Accuracy:\", acc)\n",
        "\n",
        "preds = []\n",
        "for x in X_test:\n",
        "    scores = [np.dot(w, x) for w in weights]\n",
        "    preds.append(np.argmax(scores))\n",
        "\n",
        "acc = accuracy_score(y_test, preds)\n",
        "print(\"OVR Logistic Regression Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYbqqvCW8g3A",
        "outputId": "24ea190e-14ab-410b-bfb5-ef83a3d942c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OVR Logistic Regression Accuracy: 0.8666666666666667\n",
            "OVR Logistic Regression Accuracy: 0.8666666666666667\n"
          ]
        }
      ]
    }
  ]
}